// Copyright © 2016 Alan A. A. Donovan & Brian W. Kernighan.
// License: https://creativecommons.org/licenses/by-nc-sa/4.0/

// See page 17.
//!+

// Fetchall fetches URLs in parallel and reports their times and sizes.

/*
Exercise 1.10: Find a web site that produces a large amount of data.
Investigate caching by running fetchall twice in succession to see whether the reported time changes much.
Do you get the same content each time? Modify fetchall to print its output to a ﬁle so it can be examined.
*/

package main

import (
	"fmt"
	"io"
	"io/ioutil"
	"net/http"
	"os"
	"time"
)

func main() {
	start := time.Now()
	//open a file and define the permission
	file, err := os.OpenFile("ex1.10_output.txt", os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0660)
	defer file.Close()

	if err != nil {
		fmt.Println(err)
		os.Exit(3)
	}

	ch := make(chan string)
	for _, url := range os.Args[1:] {
		go fetch(url, ch) // start a goroutine
	}
	for range os.Args[1:] {
		//
		fmt.Fprintln(file, <-ch) // print channel ch to file
	}
	fmt.Printf("%.2fs elapsed\n", time.Since(start).Seconds())
}

func fetch(url string, ch chan<- string) {
	start := time.Now()
	resp, err := http.Get(url)
	if err != nil {
		ch <- fmt.Sprint(err) // send to channel ch
		return
	}

	nbytes, err := io.Copy(ioutil.Discard, resp.Body)
	resp.Body.Close() // don't leak resources
	if err != nil {
		ch <- fmt.Sprintf("while reading %s: %v", url, err)
		return
	}
	secs := time.Since(start).Seconds()
	ch <- fmt.Sprintf("%.2fs  %7d  %s", secs, nbytes, url)
}

//!-
